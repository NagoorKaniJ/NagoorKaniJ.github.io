<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Least square residual Classifier Part - I</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">Kani Blog  <strong>Insha Allah miles to go before I sleep 
 and miles to go before I sleep</strong></a></h1>
                <nav><ul>
                    <li><a href="/category/about-me.html">About Me</a></li>
                    <li class="active"><a href="/category/blog.html">Blog</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/least-square-residual-classifier-part-i.html" rel="bookmark"
           title="Permalink to Least square residual Classifier Part - I">Least square residual Classifier Part - I<!--In the name of Almighty--></a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-06-13T18:32:00+01:00">
                Published: Tue 13 June 2017
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/nagoor-kanij.html">Nagoor Kani.J</a>
        </address>
<p>In <a href="/category/blog.html">Blog</a>.</p>

</footer><!-- /.post-info -->      <!--To display line numbers, use a path-less shebang instead of colons:

    #!python
    print("The path-less shebang syntax *will* show line numbers.")-->

<p>Residual based machine learning algorithms such as Deep Residual Neural Network,
Gradient Boosted Tree are popular and shown great promise in machine learning tasks
like computer vision.</p>
<p>This is a multi-part series in which Iâ€™m planning to cover the following residual based algorithms:</p>
<ul>
<li><strong>Least square residual classifier basen on singular value decomposition</strong>.</li>
<li><a href="{{SITEURL}}content/ONEsalamautoencoders.md"><strong>Least square residual classifier basen on simple deep autoencoders</strong></a>.</li>
<li><strong>Least square residual classifier basen on convolutional auto encoders</strong>.</li>
</ul>
<p>In this post, classification algorithm based on PCA   called <strong>least square residual classifier</strong> is introduced and applied on the MNIST handwritten digit recognition task.</p>
<h2>Description</h2>
<p>The problem is given a set of manually classified digits (training set), classify a set of unknown digits (the test set). In this digit recognition task, there are 10 digits <span class="math">\((0~\text{to}~9)\)</span> or <span class="math">\(10\)</span> classes to predict.</p>
<p>We consider each digit in the given set as a vector <span class="math">\({\mathbf{y}} \in \mathbb{R}^n\)</span>. The complete space of <span class="math">\(\mathbf{y}\)</span> is spanned by a set of <span class="math">\(n\)</span> orthonormal basis vectors <span class="math">\(\mathcal{U} = \text{span}(\mathbf{u}_1~\cdots~\mathbf{u}_n)\)</span>.</p>
<p>Each digit of specific class <span class="math">\(i=0~\cdots~9\)</span> is assumed to attracted to a certain low dimensional subspace <span class="math">\(\tilde{\mathcal{U}}_i \subset \mathcal{U}\)</span>. Hence all the digits of that specific class say 7 with reasonable accuracy can be approximated while using only the <span class="math">\(r\)</span> basis vectors that span <span class="math">\(\tilde{\mathcal{U}}_7\)</span>.  </p>
<p>The digit can be approximated by expressing it in terms of these basis vectors by stating:
    </p>
<div class="math">$$
    \begin{equation}
    \mathbf{y} =  \mathbf{U}_i~\tilde{\mathbf{y}}_i + \mathbf{r}_i
    \label{expansion}
    \end{equation}
    $$</div>
<p>
where <span class="math">\(\tilde{\mathbf{y}}_i\)</span> is the coefficient vector, <span class="math">\(\mathbf{r}_i =\)</span> residual represents the part of the <span class="math">\(\mathbf{y}\)</span> that lies orthogonal to the subspace <span class="math">\(\tilde{\mathcal{U}}_i\)</span>.</p>
<p>Thus the inner product of <span class="math">\(\mathbf{r}_i\)</span> with any of the basis vectors that span <span class="math">\(\tilde{\mathcal{U}}_i\)</span> is zero: <span class="math">\(\mathbf{U}_i^\top~\mathbf{r}_i = 0\)</span>.</p>
<p>The basis vectors of  <span class="math">\(\tilde{\mathcal{U}}_i\)</span> are collected in the matrix <span class="math">\(\mathbf{U}_i \in \mathbb{R}^{n \times r}\)</span>  and <span class="math">\(r \ll n\)</span>. SVD algorithm identifies the subspace <span class="math">\(\tilde{\mathcal{U}}_i\)</span> by computing the singular value decomposition (SVD) of the digit matrix <span class="math">\(\mathbf{X}_i\)</span> consisting of all the training digits of one kind, for example 7.</p>
<p>The SVD of <span class="math">\(\mathbf{X}_i~i=0~\cdots~9\)</span> is computed by
</p>
<div class="math">\begin{equation}
\mathbf{X}_i=\mathbf{U}_i^{\text{full}}~\Sigma_i~\mathbf{W}_i^*
\end{equation}</div>
<p>The orthonormal basis matrix <span class="math">\( \mathbf{U}_{\alpha}\)</span> for optimally approximating <span class="math">\(\mathbf{y}\)</span> of class <span class="math">\(\alpha\)</span> is given by the first <span class="math">\(r\)</span> columns of matrix <span class="math">\(\mathbf{U}_{\alpha}^{\text{full}}\)</span>.</p>
<p>We should now compute how well known an unknown digit <span class="math">\( \mathbf{y}\)</span> can be represented in the 10 different bases.  This can be done by computing the residual vector in least square problems of type</p>
<div class="math">\begin{equation}
\left\|~\mathbf{y} - \mathbf{U}_i~\tilde{\mathbf{y}}_i~\right\| % \rbrace_2
\label{leastsquare}
\end{equation}</div>
<p>Since each digit is well characterised by the reduced orthogonal basis of its own kind, we expect the norm of the residual vector computed from solving Eq. \ref{leastsquare} using orthogonal basis of the same kind is less than the residual vector computed from solving Eq. \ref{leastsquare} using orthogonal basis of the other classes.</p>
<p>More specifically, for a given unknown digit, if we compute its relative residual in all 10 bases, classify the unknown digit to the class of orthogonal bases with the minimum residual.</p>
<hr>
<hr>
<h2>Implementation</h2>
<p>Now let us implement the least square residual classifier to classify MNIST digits:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</pre></div>


<p>We now normalize all values between 0 and 1 and we now flatten the <span class="math">\(28~\times~28\)</span> images into vectors of size 784.</p>
<div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">T</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">T</span>
</pre></div>


<p>we now partition the x_train based on the ten classes and store in a list.</p>
<div class="highlight"><pre><span></span><span class="n">x_train_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">k</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_train_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span> <span class="n">components</span><span class="p">])</span>
</pre></div>


<!--  # print 'salam ', k, x_train[:, components].shape-->

<p>we now create a svd basis for each digit data samples.</p>
<div class="highlight"><pre><span></span><span class="n">U_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">x_train_list</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
    <span class="n">U_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
</pre></div>


<!--    # print 'salam singular values', U.shape -->

<p>we now select the dominant orthogonal basis and project the training digits on the reduced basis to compute the least square projection error.</p>
<div class="highlight"><pre><span></span><span class="n">mse_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">40</span><span class="p">):</span>
    <span class="n">n_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y_train_predict</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_train</span><span class="p">,</span> <span class="mi">15</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">residual_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">U_r</span> <span class="o">=</span> <span class="n">U_list</span><span class="p">[</span><span class="n">k</span><span class="p">][:,</span> <span class="p">:</span><span class="n">r</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U_r</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U_r</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>
            <span class="n">residual_list</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">residual</span>

        <span class="n">y_train_predict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">residual_list</span><span class="p">)</span>
        <span class="n">mse_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqr</span><span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">y_train_predict</span><span class="p">)))</span>
</pre></div>


<!--  # print 'salam training error', np.mean(np.sqr(y_train - y_train_predict))-->

<!--      mse_train.append(np.mean(np.sqr(y_train - y_train_predict)))  -->

<p>we now select the dominant orthogonal basis and project the test digits on the reduced basis to compute the least square projection error.</p>
<div class="highlight"><pre><span></span><span class="n">mse_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">40</span><span class="p">):</span>
    <span class="n">n_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y_test_predict</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_test</span><span class="p">,</span> <span class="mi">15</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">residual_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">U_r</span> <span class="o">=</span> <span class="n">U_list</span><span class="p">[</span><span class="n">k</span><span class="p">][:,</span> <span class="p">:</span><span class="n">r</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U_r</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U_r</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>
            <span class="n">residual_list</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">residual</span>

        <span class="n">y_test_predict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">residual_list</span><span class="p">)</span>
        <span class="n">mse_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqr</span><span class="p">(</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">y_test_predict</span><span class="p">)))</span>
</pre></div>


<!--  # print 'salam test error', np.mean(np.sqr(y_test - y_test_predict)) -->

<!--        mse_test.append(np.mean(np.sqr(y_test - y_test_predict))) -->

<p>We show the mean square error train and test results for the MNIST handwritten digit classification problem as a function of
the number of basis used in the least square residual classifier.</p>
<p><img alt="" src="/images/ONEsvdclassifierresidual.png" width="700"></p>
<p>We now show the relative residual norm for all test digit 3 and all test digit 7 in terms of all 10 bases.
We notice in the two figures, that most of the test digits 3 and 7 are best approximated in terms of their own basis.</p>
<p><img alt="" src="/images/ONEsvdclassifierresiduala.png" width="700"></p>
<p>We show in the figure below the svd approximation of digit 3 using orthogonal basis  with the different  number of basis vectors 1 to 10. In the top panel, <span class="math">\(\mathbf{U}_3\)</span> is used to compute the approximations of test digit 3 with increased basis from left to right and similarly bottom panel use <span class="math">\( \mathbf{U}_5\)</span> to compute the svd approximations of 3.</p>
<p><img alt="" src="/images/ONEsvdclassifierresidualc.png" width="700"></p>
<p>We show now how the residual depends on the number of basis vectors. In the left panel, <span class="math">\(\mathbf{U}_3\)</span> is used to compute the approximations of test digit 3 and similarly right panel use <span class="math">\(\mathbf{U}_5\)</span> to compute the svd approximations of 3.</p>
<p><img alt="" src="/images/ONEsvdclassifierresidualb.png" width="700"></p>
<p>We see that the relative residual is considerably smaller in the basis <span class="math">\(\mathbf{U}_3\)</span> than in the basis <span class="math">\(\mathbf{U}_5\)</span></p>
<hr>
<p>Finally, though the least square residual classifier is quite fast in test phase and can be suitable for real time computations, they suffer to classify ugly digits.</p>
<hr>
<p>The layout of this blog is inspired by
<a href="https://blog.keras.io/building-autoencoders-in-keras.html">Building Autoencoders in Keras</a></p>
<h3>References</h3>
<ul>
<li><a href="http://users.mai.liu.se/larel04/matrix-methods/">Matrix Methods in Data Mining and Pattern Recognition</a></li>
<li><a href="https://pdfs.semanticscholar.org/eda0/95bf52e846e8c560403d485c81e1a932eaeb.pdf">Handwritten digit classification using higher order singular value decomposition</a></li>
<li><a href="http://liu.diva-portal.org/smash/record.jsf?pid=diva2%3A18011&amp;dswid=9140">Algorithms in data mining using matrix and tensor    methods</a></li>
<li><a href="http://webstaff.itn.liu.se/~bersa48/files/thesisBerkantSavas.pdf">Analyses and Test of Handwritten Digit Recognition Algorithm</a></li>
</ul>
<hr>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>